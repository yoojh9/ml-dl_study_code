# 딥러닝의 기본 개념

딥러닝의 또 다른 이름은 심층신경망(Deep Neural Network, DNN)이다.  
이름에서 알 수 있듯이 딥러닝은 뉴런으로 구성된 레이어를 여러 개 연결해서 구상한 네트워크이며,  
네트워크를 어떻게 구성하느냐에 따라 보통 X-NN(Neural Network)으로 불린다.  
대표적인 예로 CNN(Convolutional Neural Network), RNN(Recurrent Neural Network) 등이 있다.
뇌가 사물을 이해하는 과정(CNN), 뇌가 문맥을 이해하는 과정(RNN) 등 상황에 따라 이해하는 과정을 형상화했다.

### 퍼셉트론
오래 전 뉴런 하나로 AND 연산이나 OR 연산 등을 해결하는 기술이 탄생했는데, 그 기술 이름이 바로 퍼셉트론(perceptron)이다.

### 다층 퍼셉트론
하나의 의사결정선을 그릴 수 있는 퍼셉트론으로는 XOR을 구현할 수 없었다.  
다수의 뉴런으로 구성된 여러 층 구조를 갖춘 딥러닝을 다층 퍼셉트론(Multi Layer Perceptron, MLP)이라고 부른다.  

## 1. 딥러닝의 학습
딥러닝 학습은 보통 최초 딥러닝 모델의 매개변수(가중치, 편향값)를 무작위로 부여한 후, 반복 학습을 통해 모델의 출력값을 최대한 정답과 일치하도록 매개변수를 조금씩 조정한다.

### 1) 순전파(foward Propagation)
순전파란 딥러닝에 값을 입력해서 출력을 얻는 과정을 말한다.  
순전파 과정을 거치면 출력값(y_hat)을 얻게 되고, 정답과 출력값의 차이를 구할 수 있다.  
출력값과 정답의 차이를 구하기 위한 함수를 손실함수라고 한다.

### 2) 손실함수
손실함수는 출력값과 정답의 차이를 계산한다. 보통 회귀에는 평균제곱오차를, 분류 문제에는 크로스 엔트로피를 손실함수로 사용한다.  
매개변수(가중치, 편향값)를 조절해서 손실함수의 값을 최저로 만드는 과정을 최적화(optimization)과정이라고 부르고,  
최적화 과정은 옵티마이저(optimizer)를 통해 이뤄진다. 옵티마이저는 역전파(back propagation) 과정을 수행해서 딥러닝 모델의 매개변수를 최적화한다.

### 3) 최적화
대표적인 최적화 방법은 경사하강법이다. 반복적으로 손실함수에 대한 매개변수의 미분값을 구한 후,  
그 미분값의 반대 방향으로 매개변수를 조절해 나가면 결국에는 최저 손실 함수 값에 도달한다는 이론이다.  
딥러닝 모델에는 여러 개의 매개변수가 여러 레이어에 존재하며,  
옵티마이저는 여러 레이어에 존재하는 매개변수를 조정하기 위해 역전파를 사용한다.

### 4) 역전파
옵티마이저는 손실함수의 값을 최저로 하기 위해 역전파(back propagation)를 사용해 딥러닝 모델의 모든 매개변수를 변경한다.  
이 같은 방법으로 충분히 반복 학습 과정을 거치면 모든 매개변수가 손실함수를 최저로 하는 값으로 수렴하게 된다.

### 5) 옵티마이저
#### (1) 배치 경사 하강법
배치 경사 하강법은 딥러닝 모델을 최적화하는 가장 기본적인 방법이다.  
가중치에 대한 손실함수의 1차 미분을 구하는 것만으로도 손실함수의 로컬 미니멈, 즉 출력값과 정답의 차이가 적은 딥러닝 모델을 찾을 수 있다는 것부터 획기적이다.  
모든 학습 데이터의 손실 함수를 계산한 후에만 딥러닝 모델의 매개변수가 조금씩 변경되기 때문에 로컬 미니멈까지  
매개변수를 변경하는 데 걸리는 시간이 오래 걸리고, 매개변수 조절에 필요한 계산량도 많다는 단점이 있다.  
로컬 미니멈이 여러 개 존재할 때 최적의 모델은 글로벌 미니멈의 매개변수를 가진 모델이어야 하는데,  
배치 경사하강법은 무작위로 부여된 매개변수에서부터 가장 가까운 로컬 미니멈에 멈추게 된다.  

#### (2) SGD(Stochastic gradient descent)
배치 경사하강법이 한 번 매개변수를 변경하는 데 드는 계산량이 너무 크고 시간이 오래 걸린다는 단점이 있어서 고안된 방법 중 하나이다.  
모든 데이터를 계싼해서 매개변수를 변경하는 배치 경사하강법과 달리, 하나의 데이터마다 매개변수를 변경하는 방법이다.  
비유하자면 배치 경사하강법은 꾸준히 현재의 위치에서 최저점을 향해 내려가는 거북이 같다면 SGD는 껑충껑충 뛰어다니면서 최저점을 찾는 토끼와 같다.   
모든 데이터에 대한 계산 결과를 저장해야 하는 배치 경사하강법과는 달리, 하나의 데이터에 대한 계산만 메모리에 저장하면 되므로 자원이 적은 컴퓨터에서 딥러닝 모델을 학습하기 용이하다.  
경사하강법에 비해 매개변수가 요동을 치며 변하면서 때로는 근처 로컬 미니멈을 지나쳐 글로벌 미니멈에 수렴하기도 하는 반면,  
배치 경사하강법보다 못한 매개변수로 학습이 완료될 수도 있다.  
손실함수가 매우 불규칙하고 로컬 미니멈이 많을 때는 SGD가 배치 경사하강법보다 더 나은 최적화 알고리즘일 수 있다.

#### (3) 미니 배치
배치 경사하강법은 너무 느리고, 리소스도 많이 사용하고, SGD는 확실히 빨리 학습되지만 모델이 최적화가 안 돼 있는 경우가 많다. 이러한 두 방법의 절충안이 미니 배치이다.  
전체 데이터를 계싼해서 매개변수를 변경하는 대신 정해진 양만큼 계산해서 매개변수를 최적화 하는 방법이다.  
최근 딥러닝은 학습에서는 대다수 미니 배치를 사용한다. 

- epoch(주기): 전체 데이터 학습을 몇 회 진행할 지
- batch size: 매개변수 조정을 위해 한 번에 처리하는 데이터의 양.
- step: 미니 배치를 사용해 매개변수가 조정되는 순간마다 스텝이라고 한다. 1000개의 데이터가 있고 배치 사이즈가 100일 경우 총 10번의 스텝으로 10번의 매개변수 조정이 이루어지며, 이 10번의 스텝은 바로 1개의 주기가 된다.

#### (4) 모멘텀
단순히 1차 미분만으로 최적화를 진행하다 보면 당연히 근처의 로컬 미니멈으로 딥러닝 모델이 학습된다.  
이를 개선하기 위한 방안으로 물리학을 응용한 방법이 있는데, 이것이 바로 모멘텀이다.  
마치 공을 언덕에서 굴렀을 때 위치 에너지와 운동 에너지의 영향으로 공이 가던 방향으로 힘을 받아 최초 로컬 미니멈에 머무르지 않고,  
더 낮은 로컬 미니멈까지 공이 굴러갈 수 있다는 이론이다. 단, 모멘텀 옵티마이저 역시 글로벌 미니멈으로 최적화된다는 보정은 없다.

### 6) 학습률
경사하강법 공식에는 항상 학습률(learning rate)이 있다.  
최초 무작위로 설정된 매개변수 위치에서는 로컬 미니멈에서 멀리 있을 확률이 크므로 학습률을 크게 설정해서 학습 속도를 늘리고,  
모델이 어느정도 학습되면 매개변수가 로컬 미니멈에 수렴하도록 학습률을 작게 조절하면 효율적으로 학습할 수 있다.  
전통적인 배치 경사하강법의 경우, 최초 고정된 학습률로 학습이 종료될 떄까지 유지하지만 최근 들어 많은 연구와 함께 시간 기반 학습률 조정(time based decay), 스텝 기반 학습률 조정(step decay) 등, 학습 중간에 학습률을 조정하는 방법이 많이 생기고 있다.   
이처럼 중간에 학습률을 조정하는 방식을 decay라고 한다.

#### Adagrad
decay를 사용한 학습률 조정에는 한계가 있다. 첫째로 딥러닝의 상당히 많은 가중치에 동일한 학습률을 적용하게 된다.  
둘째로, 학습률 조정이 탄력적이지 않고 최초에는 로컬 미니멈에서 멀다가 학습이 어느정도 지난 후 로컬 미니멈에 가까이 왔다는 가설이 언제나 맞지는 않다.  
Adagrad는 각 매개변수에 각기 다른 학습률을 적용하고, 빈번히 변화가 찾아오는 가중치는 학습률이 작게 설정되고, 변화가 적은 가중치는 학습률이 높게 설정되는 옵티마이저이다.  
이러한 Adagrad의 특성은 자연어 처리에서 장점이 드러난다. 자연어 처리에서 단어는 보통 원 핫 인코딩으로 벡터화되며, 이 벡터에서 대다수는 의미 없는 0이고, 하나의 비트만 1이므로 빈번히 나타나는 0들에 대한 학습률은 적게하고, 극소수로 나타나는 1의 변화에만 학습률을 높이면 학습이 훨씬 효율적이다

#### Adam
Adam 옵티아미저는 Adagrad의 학습률 자율 조정과 모멘텀의 효율적인 매개변수 변경 알고리즘을 조합한 알고리즘이다. 이러한 이유로 최근 들어 가장 많이 활용되는 옵티마이저이다.

## 2. 딥러닝의 과대적합

### 1) 드롭아웃
드롭아웃은 매개변수 중 일정량을 학습 중간마다 무작위로 사용하지 않는 방법이다. 이러한 방법은 모델의 분산을 줄이는 데 효과적이다.  
머신러닝 모델은 분산을 줄이고 편향을 높히면 과대적합의 위험이 감소된다.  
랜덤 포레스트의 추론 시 다양하게 학습된 소규모 의사결정 트리가 투표를 통해 더 나은 결론을 내리듯,  
딥러닝 학습 역시 드롭아웃을 사용해 소규모 노드들로 다양하게 여러 번 학습되어 결과적으로 과대적합의 위험을 줄일 수 있다.

### 2) 조기종료
오버피팅을 줄일 수 있는 두 번째 방법으로 조기 종료(Early Stopping)가 있다. 학습을 진행할 때 최대 학습 반복 횟수를 설정하되,  
모델이 과대적합 될 소지가 있는 경우 학습을 중단하고 지금까지 학습된 모델 중에서 최고의 모델을 선정해야 한다.  
검증 정확도가 꾸준히 떨어지는 시점이 발견되면 그 즉시 학습을 중단하고, 지금까지 학습된 모델 중 최고의 검증 정확도를 보이는 모델을 선정해서 테스트 데이터로 테스트를 진행한다.
